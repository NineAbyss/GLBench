accelerate==0.21.0
anthropic==0.34.2
bidict==0.23.1
cacheflow==0.3
chunkdot==0.5.0
deepspeed==0.15.1
easydict==1.13
einops==0.8.0
faiss_gpu==1.7.2
fastapi==0.115.0
fastchat==0.1.0
flash_attn==2.6.3
ftfy==6.2.3
gdown==5.2.0
gradio==4.44.0
httpx==0.27.2
huggingface_hub==0.19.4
hydra-core==1.3.2
ipython==8.12.3
joblib==1.3.2
langchain==0.3.0
markdown2==2.5.0
matplotlib==3.8.2
networkx==3.2.1
nh3==0.2.18
numba==0.60.0
numpy==2.1.1
ogb==1.3.6
omegaconf==2.3.0
openai==0.28.0
packaging==24.1
pandas==1.5.3
pecos==0.3.1
peft==0.7.1
Pillow==10.1.0
Pillow==10.4.0
plotly==4.14.3
polyglot==16.7.4
prompt_toolkit==3.0.43
psutil==5.9.6
pycld2==0.41
pydantic==2.9.2
pytorch_lightning==2.1.2
pytz==2023.3.post1
PyYAML==6.0.1
PyYAML==6.0.2
rank_bm25==0.2.2
ray==2.37.0
rdkit==2023.3.3
regex==2023.10.3
Requests==2.32.3
rich==13.8.1
scikit_learn==1.3.2
scipy==1.14.1
seaborn==0.11.2
sentence_transformers==2.2.2
shortuuid==1.0.13
tables==3.10.1
tenacity==9.0.0
termcolor==2.4.0
tiktoken==0.7.0
# Please check your cuda verison
torch==2.1.1
torch_geometric==2.6.0
torch_scatter==2.1.2+pt21cu121
torch_sparse==0.6.18+pt21cu121
torchmetrics==1.2.0
tqdm==4.66.1
transformers==4.35.2
# We suggest using transformers==4.17.0 when benchmarking GLEM
uvicorn==0.30.6
vertexai==1.68.0
yacs==0.1.8
