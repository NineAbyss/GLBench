{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset, sub_dataset):\n",
    "    # read raw data\n",
    "    with open(f'/home/yuhanli/GLBench/models/alignment/Patton/data/{sub_dataset}/papers_bert.json') as f:\n",
    "        data = {}\n",
    "        readin = f.readlines()\n",
    "        for line in tqdm(readin, desc=\"Loading Data...\"):\n",
    "            tmp = eval(line.strip())\n",
    "            data[tmp['paper']] = tmp\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_co_author_data(data):\n",
    "    co_author = defaultdict(set)\n",
    "    id2author = defaultdict(set)\n",
    "    for idd in tqdm(data, desc=\"Generating co author...\"):\n",
    "        if 'author' not in data[idd] or not data[idd]['author']: continue\n",
    "        for a in data[idd]['author']:\n",
    "            co_author[a].add(idd)\n",
    "            id2author[idd].add(a)\n",
    "\n",
    "    # print(len(co_author))\n",
    "    # avgv = 0\n",
    "    # mx = 0\n",
    "    # mm = float('inf')\n",
    "    # for k in co_author:\n",
    "    #     avgv += len(co_author[k])\n",
    "    #     mx = max(mx, len(co_author[k]))\n",
    "    #     mm = min(mm, len(co_author[k]))\n",
    "    # print(avgv, len(co_author), avgv/len(co_author), mx, mm)\n",
    "    return co_author, id2author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(co_author):\n",
    "    co_author_pairs = set()\n",
    "    for author in tqdm(co_author, desc=\"Generating pairs...\"):\n",
    "        cur_v = list(co_author[author])\n",
    "        for i in range(len(cur_v)):\n",
    "            for j in range(i+1, len(cur_v)):\n",
    "                if cur_v[i] != cur_v[j]:\n",
    "                    if (cur_v[i], cur_v[j]) not in co_author_pairs:\n",
    "                        co_author_pairs.add( (cur_v[i], cur_v[j]) )\n",
    "    return list(co_author_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pairs(co_author_pair_list, n_train=128, n_val=128, n_test=10000):\n",
    "\n",
    "    def do_sample(num, pos, tgt):\n",
    "        for i in tqdm(range(num), desc=\"Sampling pairs...\"):\n",
    "            tgt.append(co_author_pair_list[idx[pos]])\n",
    "            pos += 1\n",
    "        return pos\n",
    "\n",
    "    train_pairs = []\n",
    "    val_pairs = []\n",
    "    test_pairs = []\n",
    "    idx = np.random.permutation(len(co_author_pair_list))\n",
    "    pos = 0\n",
    "    pos = do_sample(n_train, pos, train_pairs)\n",
    "    pos = do_sample(n_val, pos, val_pairs)\n",
    "    do_sample(n_test, pos, test_pairs)\n",
    "    return train_pairs, val_pairs, test_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_neighbors(author2ids, id2author, train_pairs, val_pairs, test_pairs, n_neigbhor=5):\n",
    "\n",
    "\n",
    "    def do_sample(candidates):\n",
    "        if len(candidates) <= n_neigbhor:\n",
    "            return candidates\n",
    "        ret = set()\n",
    "        while len(ret) < n_neigbhor:\n",
    "            cur = np.random.randint(len(candidates))\n",
    "            ret.add(candidates[cur])\n",
    "        return list(ret)\n",
    "    \n",
    "    def get_possible_neighbor(authors):\n",
    "        ret = set()\n",
    "        for a in authors:\n",
    "            for paper in author2ids[a]:\n",
    "                ret.add(paper)\n",
    "        return ret\n",
    "    \n",
    "    no_sample = defaultdict(set)\n",
    "    for cur_pairs in [val_pairs, test_pairs]:\n",
    "        for p in cur_pairs:\n",
    "            no_sample[p[0]].add(p[1]) # We don't want to sample these in train\n",
    "            no_sample[p[1]].add(p[0])\n",
    "    \n",
    "    train_with_neigbhor = []\n",
    "    val_with_neighbor = []\n",
    "    test_with_neighbor = []\n",
    "    # Sample train neighbors\n",
    "    for pair in tqdm(train_pairs, desc=\"Sample Train Neigbhors\"):\n",
    "        p,q = pair\n",
    "        possible_neihbors_p = get_possible_neighbor(id2author[p])\n",
    "        possible_neihbors_q = get_possible_neighbor(id2author[q])\n",
    "        cur_sample_p = possible_neihbors_p - no_sample[p]\n",
    "        cur_sample_q = possible_neihbors_q - no_sample[q]\n",
    "        cur_sample_p.remove(q)\n",
    "        cur_sample_p.remove(p)\n",
    "        cur_sample_q.remove(p)\n",
    "        cur_sample_q.remove(q)\n",
    "        p_n = do_sample(list(cur_sample_p))\n",
    "        q_n = do_sample(list(cur_sample_q))\n",
    "        train_with_neigbhor.append((p, q, p_n, q_n))\n",
    "    \n",
    "    for pair in tqdm(val_pairs, desc=\"Sample Val Neigbhors\"):\n",
    "        p,q = pair\n",
    "        possible_neihbors_p = get_possible_neighbor(id2author[p])\n",
    "        possible_neihbors_q = get_possible_neighbor(id2author[q])\n",
    "        possible_neihbors_p.remove(p)\n",
    "        possible_neihbors_q.remove(q)\n",
    "        p_n = do_sample(list(possible_neihbors_p))\n",
    "        q_n = do_sample(list(possible_neihbors_q))\n",
    "        val_with_neighbor.append((p, q, p_n, q_n))\n",
    "\n",
    "    for pair in tqdm(test_pairs, desc=\"Sample Test Neigbhors\"):\n",
    "        p,q = pair\n",
    "        possible_neihbors_p = get_possible_neighbor(id2author[p])\n",
    "        possible_neihbors_q = get_possible_neighbor(id2author[q])\n",
    "        possible_neihbors_p.remove(p)\n",
    "        possible_neihbors_q.remove(q)\n",
    "        p_n = do_sample(list(possible_neihbors_p))\n",
    "        q_n = do_sample(list(possible_neihbors_q))\n",
    "        test_with_neighbor.append((p, q, p_n, q_n))\n",
    "    return train_with_neigbhor, val_with_neighbor, test_with_neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_dump(data, tuples, path):\n",
    "    print(\"Dump data to %s\" % path)\n",
    "    with open(path, 'w') as fout:\n",
    "        for t in tqdm(tuples, desc=\"Processing %s\" % path.split('/')[-1]):\n",
    "            q, k, q_n, k_n = t\n",
    "            cur = {}\n",
    "            cur['q_text'] = data[q]['title']\n",
    "            cur['q_n_text'] = []\n",
    "            for paper in q_n:\n",
    "                cur['q_n_text'].append(data[paper]['title'])\n",
    "            cur['k_text'] = data[k]['title']\n",
    "            cur['k_n_text'] = []\n",
    "            for paper in k_n:\n",
    "                cur['k_n_text'].append(data[paper]['title'])\n",
    "            fout.write(json.dumps(cur)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/yuhanli/GLBench/models/alignment/Patton/data/MAG/Mathematics/papers_bert.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1972069/4189189136.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtask_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'co-author'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_neighbor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcur_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1972069/1799333125.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(dataset, sub_dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# read raw data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/home/yuhanli/GLBench/models/alignment/Patton/data/{dataset}/{sub_dataset}/papers_bert.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mreadin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/yuhanli/GLBench/models/alignment/Patton/data/MAG/Mathematics/papers_bert.json'"
     ]
    }
   ],
   "source": [
    "datasets = ['MAG'][0]\n",
    "sub_datasets = ['Mathematics'][0]\n",
    "base_dir = '/shared/data3/wentao4/transfernet/data/'\n",
    "task_name = 'co-author'\n",
    "n_neighbor = 5\n",
    "cur_d = load_data(datasets, sub_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating co author...: 100%|██████████| 178670/178670 [00:03<00:00, 57977.93it/s] \n"
     ]
    }
   ],
   "source": [
    "author2ids, id2author = generate_co_author_data(cur_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs...: 100%|██████████| 135247/135247 [00:01<00:00, 121495.48it/s]\n"
     ]
    }
   ],
   "source": [
    "pairs = generate_pairs(author2ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling pairs...: 100%|██████████| 128/128 [00:00<00:00, 434713.29it/s]\n",
      "Sampling pairs...: 100%|██████████| 128/128 [00:00<00:00, 409825.12it/s]\n",
      "Sampling pairs...: 100%|██████████| 10000/10000 [00:00<00:00, 482092.83it/s]\n"
     ]
    }
   ],
   "source": [
    "train, val, test = sample_pairs(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample Train Neigbhors: 100%|██████████| 128/128 [00:00<00:00, 8623.60it/s]\n",
      "Sample Val Neigbhors: 100%|██████████| 128/128 [00:00<00:00, 10772.54it/s]\n",
      "Sample Test Neigbhors: 100%|██████████| 10000/10000 [00:00<00:00, 15125.45it/s]\n"
     ]
    }
   ],
   "source": [
    "train_n, val_n, test_n = sample_neighbors(author2ids, id2author, train, val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dump data to /shared/data3/wentao4/transfernet/data/MAG/Economics/co-author/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train.jsonl: 100%|██████████| 128/128 [00:00<00:00, 21529.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dump data to /shared/data3/wentao4/transfernet/data/MAG/Economics/co-author/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val.jsonl: 100%|██████████| 128/128 [00:00<00:00, 30384.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dump data to /shared/data3/wentao4/transfernet/data/MAG/Economics/co-author/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test.jsonl: 100%|██████████| 10000/10000 [00:00<00:00, 45218.00it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(base_dir, datasets, sub_datasets, task_name)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "convert_and_dump(cur_d, train_n, os.path.join(save_dir, 'train.jsonl'))\n",
    "convert_and_dump(cur_d, val_n, os.path.join(save_dir, 'val.jsonl'))\n",
    "convert_and_dump(cur_d, test_n, os.path.join(save_dir, 'test.jsonl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "79184ab466d025e93aa564ab8198c1ff82722e8819dc6e18ef84955c4b52b62a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
