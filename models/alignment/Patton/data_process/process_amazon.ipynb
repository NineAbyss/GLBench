{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f795fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "667e1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf349bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'amazon' \n",
    "sub_dataset='sports'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3baddf92",
   "metadata": {},
   "source": [
    "# Generate Pretraining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba7e9e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 532197/532197 [01:11<00:00, 7454.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# read raw data\n",
    "with open(f'amazon/{sub_dataset}/product.json') as f:\n",
    "    raw_data = {}\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        #data.append(json.loads(line))\n",
    "        #data.append(eval(line.strip()))\n",
    "        tmp = eval(line.strip())\n",
    "        raw_data[tmp['asin']] = tmp\n",
    "#random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "671786b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 532197/532197 [00:01<00:00, 325071.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# statistics on label names\n",
    "label_name_stat = defaultdict(int)\n",
    "\n",
    "for did in tqdm(raw_data):\n",
    "    sample = raw_data[did]\n",
    "    c_list = list(set(sum(sample['categories'], [])))\n",
    "    for c in c_list:\n",
    "        label_name_stat[c] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab1314a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique labels:3034\n"
     ]
    }
   ],
   "source": [
    "# read label name dict\n",
    "\n",
    "label_name_dict = {}\n",
    "label_name_set = set()\n",
    "label_name2id_dict = {}\n",
    "\n",
    "for n in label_name_stat:\n",
    "    if label_name_stat[n] > int(0.5 * len(raw_data)):\n",
    "        continue\n",
    "\n",
    "    label_name_dict[len(label_name_dict)] = n\n",
    "    label_name_set.add(n)\n",
    "    label_name2id_dict[n] = len(label_name_dict) - 1\n",
    "\n",
    "print(f'Num of unique labels:{len(label_name_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "556159f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 532197/532197 [00:00<00:00, 964799.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# filter item with no text\n",
    "\n",
    "data = {}\n",
    "for idd in tqdm(raw_data):\n",
    "    if 'title' in raw_data[idd]:\n",
    "        data[idd] = raw_data[idd]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63d00a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 529901/529901 [00:04<00:00, 117485.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# filter related\n",
    "\n",
    "idd_set = set(list(data.keys()))\n",
    "\n",
    "for idd in tqdm(data):\n",
    "    if 'related' not in data[idd]:\n",
    "        continue\n",
    "        \n",
    "    if 'also_bought' in data[idd]['related']:\n",
    "        data[idd]['related']['also_bought'] = list(set(data[idd]['related']['also_bought']) & idd_set)\n",
    "        \n",
    "    if 'also_viewed' in data[idd]['related']:\n",
    "        data[idd]['related']['also_viewed'] = list(set(data[idd]['related']['also_viewed']) & idd_set)\n",
    "        \n",
    "    if 'bought_together' in data[idd]['related']:\n",
    "        data[idd]['related']['bought_together'] = list(set(data[idd]['related']['bought_together']) & idd_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f8fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text processing function\n",
    "def text_process(text):\n",
    "    p_text = ' '.join(text.split('\\r\\n'))\n",
    "    p_text = ' '.join(p_text.split('\\n\\r'))\n",
    "    p_text = ' '.join(p_text.split('\\n'))\n",
    "    p_text = ' '.join(p_text.split('\\t'))\n",
    "    p_text = ' '.join(p_text.split('\\rm'))\n",
    "    p_text = ' '.join(p_text.split('\\r'))\n",
    "    p_text = ''.join(p_text.split('$'))\n",
    "    p_text = ''.join(p_text.split('*'))\n",
    "\n",
    "    return p_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e606b4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1502696/1502696 [00:03<00:00, 476815.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average also bought:4.177224135819887, averagte also viewed:6.8697906961887165, average bought together:0.3685841980014587.\n",
      "also bought items:661544, also viewed items:959912, bought together items:572456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# average edge\n",
    "\n",
    "also_bought_cnt = 0\n",
    "also_viewed_cnt = 0\n",
    "bought_together_cnt = 0\n",
    "\n",
    "also_bought_item = {}\n",
    "also_viewed_item = {}\n",
    "bought_together_item = {}\n",
    "\n",
    "\n",
    "for idd in tqdm(data):\n",
    "    if 'related' not in data[idd] or 'title' not in data[idd]:\n",
    "        continue\n",
    "        \n",
    "    if 'also_bought' in data[idd]['related']:        \n",
    "        also_bought_cnt += len(data[idd]['related']['also_bought'])\n",
    "        also_bought_item[idd] = data[idd]\n",
    "        \n",
    "    if 'also_viewed' in data[idd]['related']:\n",
    "        also_viewed_cnt += len(data[idd]['related']['also_viewed'])\n",
    "        also_viewed_item[idd] = data[idd]\n",
    "        \n",
    "    if 'bought_together' in data[idd]['related']:\n",
    "        bought_together_cnt += len(data[idd]['related']['bought_together'])\n",
    "        bought_together_item[idd] = data[idd]\n",
    "\n",
    "print(f'average also bought:{also_bought_cnt/len(data)}, averagte also viewed:{also_viewed_cnt/len(data)}, average bought together:{bought_together_cnt/len(data)}.')\n",
    "print(f'also bought items:{len(also_bought_item)}, also viewed items:{len(also_viewed_item)}, bought together items:{len(bought_together_item)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "691d2a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 959912/959912 [00:31<00:00, 30873.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test size:7876427,902293,1402757\n",
      "Train/Val/Test avg:8.205363616664862,0.939974706014718,1.4613391644234055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## split train/val/test as 8:1:1\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "train_pairs = []\n",
    "val_pairs = []\n",
    "test_pairs = []\n",
    "train_pair_set = set()\n",
    "item_id2idx = {}\n",
    "train_neighbor = defaultdict(list)\n",
    "\n",
    "for iid in tqdm(also_viewed_item):\n",
    "    if iid not in item_id2idx:\n",
    "        item_id2idx[iid] = len(item_id2idx)\n",
    "    \n",
    "    also_viewed = also_viewed_item[iid]['related']['also_viewed']\n",
    "    random.shuffle(also_viewed)\n",
    "    \n",
    "    for i in range(int(len(also_viewed)*0.8)):\n",
    "        train_pairs.append((iid,also_viewed[i]))\n",
    "        train_pair_set.add((iid,also_viewed[i]))\n",
    "        train_pair_set.add((also_viewed[i],iid))\n",
    "        \n",
    "        # add to item_id2idx\n",
    "        if also_viewed[i] not in item_id2idx:\n",
    "            item_id2idx[also_viewed[i]] = len(item_id2idx)\n",
    "\n",
    "        # add to train_user_neighbor/train_item_neighbor\n",
    "        train_neighbor[iid].append(also_viewed[i])\n",
    "\n",
    "    for i in range(int(len(also_viewed)*0.8),int(len(also_viewed)*0.9)):\n",
    "        if (iid,also_viewed[i]) in train_pair_set:\n",
    "            continue\n",
    "        val_pairs.append((iid,also_viewed[i]))\n",
    "        assert (iid,also_viewed[i]) not in train_pair_set\n",
    "\n",
    "        # add to item_id2idx\n",
    "        if also_viewed[i] not in item_id2idx:\n",
    "            item_id2idx[also_viewed[i]] = len(item_id2idx)\n",
    "        \n",
    "    for i in range(int(len(also_viewed)*0.9),len(also_viewed)):\n",
    "        if (iid,also_viewed[i]) in train_pair_set:\n",
    "            continue\n",
    "        test_pairs.append((iid,also_viewed[i]))\n",
    "        assert (iid,also_viewed[i]) not in train_pair_set\n",
    "        \n",
    "        # add to item_id2idx\n",
    "        if also_viewed[i] not in item_id2idx:\n",
    "            item_id2idx[also_viewed[i]] = len(item_id2idx)\n",
    "        \n",
    "print(f'Train/Val/Test size:{len(train_pairs)},{len(val_pairs)},{len(test_pairs)}')\n",
    "print(f'Train/Val/Test avg:{len(train_pairs)/len(also_viewed_item)},{len(val_pairs)/len(also_viewed_item)},{len(test_pairs)/len(also_viewed_item)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89099ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_neighbor_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0594128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 279788/279788 [00:03<00:00, 89284.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# save all the text on node in the graph\n",
    "\n",
    "node_id_set = set()\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/corpus.txt','w') as fout:    \n",
    "    for iid in tqdm(also_viewed_item):\n",
    "        also_viewed = also_viewed_item[iid]['related']['also_viewed']\n",
    "        \n",
    "        # save iid text\n",
    "        if iid not in node_id_set:\n",
    "            node_id_set.add(idd)\n",
    "            if 'description' in data[iid]:\n",
    "                tmp_text = text_process(data[iid]['title'] + ' ' + data[iid]['description'])\n",
    "            else:\n",
    "                tmp_text = text_process(data[iid]['title'])\n",
    "            if tmp_text not in ['', 'null']:\n",
    "                fout.write(idd+'\\t'+tmp_text+'\\n')\n",
    "    \n",
    "        # save neighbor\n",
    "        for iid_n in also_viewed:\n",
    "            if iid_n not in node_id_set:\n",
    "                node_id_set.add(iid_n)\n",
    "                if 'description' in data[iid_n]:\n",
    "                    tmp_text = text_process(data[iid_n]['title'] + ' ' + data[iid_n]['description'])\n",
    "                else:\n",
    "                    tmp_text = text_process(data[iid_n]['title'])\n",
    "                if tmp_text not in ['', 'null']:\n",
    "                    fout.write(iid_n+'\\t'+tmp_text+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6e668bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4479834/4479834 [16:43<00:00, 4462.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate and save train file\n",
    "\n",
    "random.seed(0)\n",
    "sample_neighbor_num = 5\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/train.text.jsonl','w') as fout:\n",
    "    for (q, k) in tqdm(train_pairs):\n",
    "        \n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[q]))\n",
    "        k_n_pool = set(deepcopy(train_neighbor[k]))\n",
    "\n",
    "        if k in q_n_pool:\n",
    "            q_n_pool.remove(k)\n",
    "        if q in k_n_pool:\n",
    "            k_n_pool.remove(q)\n",
    "\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        k_n_pool = list(k_n_pool)\n",
    "        random.shuffle(q_n_pool)\n",
    "        random.shuffle(k_n_pool)\n",
    "        \n",
    "        # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num-len(q_n_pool))\n",
    "        \n",
    "        if len(k_n_pool) >= sample_neighbor_num:\n",
    "            k_samples = k_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            k_samples = k_n_pool + [-1] * (sample_neighbor_num-len(k_n_pool))\n",
    "        \n",
    "        # prepare for writing file\n",
    "        if 'description' in data[q]:\n",
    "            q_text = text_process(data[q]['title'] + ' ' + data[q]['description'])\n",
    "        else:\n",
    "            q_text = text_process(data[q]['title'])\n",
    "        q_n_text = []\n",
    "        for q_n in q_samples:\n",
    "            if q_n == -1:\n",
    "                q_n_text.append('')\n",
    "            elif 'description' in data[q_n]:\n",
    "                q_n_text.append(text_process(data[q_n]['title'] + ' ' + data[q_n]['description']))\n",
    "            else:\n",
    "                q_n_text.append(text_process(data[q_n]['title']))\n",
    "\n",
    "        if 'description' in data[k]:\n",
    "            k_text = text_process(data[k]['title'] + ' ' + data[k]['description'])\n",
    "        else:\n",
    "            k_text = text_process(data[k]['title'])\n",
    "        k_n_text = []\n",
    "        for k_n in k_samples:\n",
    "            if k_n == -1:\n",
    "                k_n_text.append('')\n",
    "            elif 'description' in data[k_n]:\n",
    "                k_n_text.append(text_process(data[k_n]['title'] + ' ' + data[k_n]['description']))\n",
    "            else:\n",
    "                k_n_text.append(text_process(data[k_n]['title']))\n",
    "        \n",
    "        #fout.write(q_line+'\\t'+k_line+'\\n')\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':q_text,\n",
    "            'q_n_text':q_n_text,\n",
    "            'k_text':k_text,\n",
    "            'k_n_text':k_n_text,\n",
    "        })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1af48f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 457776/457776 [01:40<00:00, 4559.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate and save val file (make sure to delete items that are not in train set)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/val.text.jsonl','w') as fout:\n",
    "    for (q, k) in tqdm(val_pairs):\n",
    "        \n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[q]))\n",
    "        k_n_pool = set(deepcopy(train_neighbor[k]))\n",
    "\n",
    "        if k in q_n_pool:\n",
    "            q_n_pool.remove(k)\n",
    "        if q in k_n_pool:\n",
    "            k_n_pool.remove(q)\n",
    "\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        k_n_pool = list(k_n_pool)\n",
    "        random.shuffle(q_n_pool)\n",
    "        random.shuffle(k_n_pool)\n",
    "        \n",
    "        # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num-len(q_n_pool))\n",
    "        \n",
    "        if len(k_n_pool) >= sample_neighbor_num:\n",
    "            k_samples = k_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            k_samples = k_n_pool + [-1] * (sample_neighbor_num-len(k_n_pool))\n",
    "        \n",
    "        # prepare for writing file\n",
    "        if 'description' in data[q]:\n",
    "            q_text = text_process(data[q]['title'] + ' ' + data[q]['description'])\n",
    "        else:\n",
    "            q_text = text_process(data[q]['title'])\n",
    "        q_n_text = []\n",
    "        for q_n in q_samples:\n",
    "            if q_n == -1:\n",
    "                q_n_text.append('')\n",
    "            elif 'description' in data[q_n]:\n",
    "                q_n_text.append(text_process(data[q_n]['title'] + ' ' + data[q_n]['description']))\n",
    "            else:\n",
    "                q_n_text.append(text_process(data[q_n]['title']))\n",
    "\n",
    "        if 'description' in data[k]:\n",
    "            k_text = text_process(data[k]['title'] + ' ' + data[k]['description'])\n",
    "        else:\n",
    "            k_text = text_process(data[k]['title'])\n",
    "        k_n_text = []\n",
    "        for k_n in k_samples:\n",
    "            if k_n == -1:\n",
    "                k_n_text.append('')\n",
    "            elif 'description' in data[k_n]:\n",
    "                k_n_text.append(text_process(data[k_n]['title'] + ' ' + data[k_n]['description']))\n",
    "            else:\n",
    "                k_n_text.append(text_process(data[k_n]['title']))\n",
    "        \n",
    "        #fout.write(q_line+'\\t'+k_line+'\\n')\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':q_text,\n",
    "            'q_n_text':q_n_text,\n",
    "            'k_text':k_text,\n",
    "            'k_n_text':k_n_text,\n",
    "        })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b14bc30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 589610/589610 [02:13<00:00, 4401.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate and save test file (make sure to delete items that are not in train set)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/test.text.jsonl','w') as fout:\n",
    "    for (q, k) in tqdm(test_pairs):\n",
    "        \n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[q]))\n",
    "        k_n_pool = set(deepcopy(train_neighbor[k]))\n",
    "\n",
    "        if k in q_n_pool:\n",
    "            q_n_pool.remove(k)\n",
    "        if q in k_n_pool:\n",
    "            k_n_pool.remove(q)\n",
    "\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        k_n_pool = list(k_n_pool)\n",
    "        random.shuffle(q_n_pool)\n",
    "        random.shuffle(k_n_pool)\n",
    "        \n",
    "        # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num-len(q_n_pool))\n",
    "        \n",
    "        if len(k_n_pool) >= sample_neighbor_num:\n",
    "            k_samples = k_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            k_samples = k_n_pool + [-1] * (sample_neighbor_num-len(k_n_pool))\n",
    "        \n",
    "        # prepare for writing file\n",
    "        if 'description' in data[q]:\n",
    "            q_text = text_process(data[q]['title'] + ' ' + data[q]['description'])\n",
    "        else:\n",
    "            q_text = text_process(data[q]['title'])\n",
    "        q_n_text = []\n",
    "        for q_n in q_samples:\n",
    "            if q_n == -1:\n",
    "                q_n_text.append('')\n",
    "            elif 'description' in data[q_n]:\n",
    "                q_n_text.append(text_process(data[q_n]['title'] + ' ' + data[q_n]['description']))\n",
    "            else:\n",
    "                q_n_text.append(text_process(data[q_n]['title']))\n",
    "\n",
    "        if 'description' in data[k]:\n",
    "            k_text = text_process(data[k]['title'] + ' ' + data[k]['description'])\n",
    "        else:\n",
    "            k_text = text_process(data[k]['title'])\n",
    "        k_n_text = []\n",
    "        for k_n in k_samples:\n",
    "            if k_n == -1:\n",
    "                k_n_text.append('')\n",
    "            elif 'description' in data[k_n]:\n",
    "                k_n_text.append(text_process(data[k_n]['title'] + ' ' + data[k_n]['description']))\n",
    "            else:\n",
    "                k_n_text.append(text_process(data[k_n]['title']))\n",
    "        \n",
    "        #fout.write(q_line+'\\t'+k_line+'\\n')\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':q_text,\n",
    "            'q_n_text':q_n_text,\n",
    "            'k_text':k_text,\n",
    "            'k_n_text':k_n_text,\n",
    "        })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22662de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save side files\n",
    "pickle.dump([sample_neighbor_num],open(f'data_dir/{dataset}/{sub_dataset}/neighbor_sampling.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2092165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save neighbor file\n",
    "pickle.dump(train_neighbor,open(f'data_dir/{dataset}/{sub_dataset}/neighbor/train_neighbor.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6853c916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 279788/279788 [00:22<00:00, 12297.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# save node labels\n",
    "random.seed(0)\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/nc/node_classification.jsonl','w') as fout:\n",
    "    for q in tqdm(also_viewed_item):\n",
    "        \n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[q]))\n",
    "\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        random.shuffle(q_n_pool)\n",
    "        \n",
    "        # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num-len(q_n_pool))\n",
    "        \n",
    "        # prepare for writing file\n",
    "        if 'description' in data[q]:\n",
    "            q_text = text_process(data[q]['title'] + ' ' + data[q]['description'])\n",
    "        else:\n",
    "            q_text = text_process(data[q]['title'])\n",
    "        q_n_text = []\n",
    "        for q_n in q_samples:\n",
    "            if q_n == -1:\n",
    "                q_n_text.append('')\n",
    "            elif 'description' in data[q_n]:\n",
    "                q_n_text.append(text_process(data[q_n]['title'] + ' ' + data[q_n]['description']))\n",
    "            else:\n",
    "                q_n_text.append(text_process(data[q_n]['title']))\n",
    "        \n",
    "        \n",
    "        label_names_list = list(set(sum(data[q]['categories'], [])))\n",
    "        label_names_list = [n for n in label_names_list if n in label_name2id_dict]\n",
    "        label_ids_list = [label_name2id_dict[n] for n in label_names_list]\n",
    "        \n",
    "        if len(label_ids_list) != 0:\n",
    "            fout.write(json.dumps({\n",
    "                'q_text':q_text,\n",
    "                'q_n_text':q_n_text,\n",
    "                'labels':label_ids_list,\n",
    "                'label_names':label_names_list\n",
    "            })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3076a2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 309311/309311 [00:00<00:00, 706064.65it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 309311/309311 [00:03<00:00, 88608.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61862/61862 [00:00<00:00, 86549.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61863/61863 [00:00<00:00, 91207.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate self constrastive pretraining\n",
    "\n",
    "corpus_list = []\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/corpus.txt') as f:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        tmp = line.strip().split('\\t')\n",
    "        corpus_list.append(tmp[1])\n",
    "        \n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/self-train/train.text.jsonl','w') as fout:\n",
    "    for dd in tqdm(corpus_list):\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':dd,\n",
    "            'q_n_text':[''],\n",
    "            'k_text':dd,\n",
    "            'k_n_text':[''],\n",
    "        })+'\\n')\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/self-train/val.text.jsonl','w') as fout:\n",
    "    for dd in tqdm(corpus_list[:int(0.2*len(corpus_list))]):\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':dd,\n",
    "            'q_n_text':[''],\n",
    "            'k_text':dd,\n",
    "            'k_n_text':[''],\n",
    "        })+'\\n')\n",
    "        \n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/self-train/test.text.jsonl','w') as fout:\n",
    "    for dd in tqdm(corpus_list[int(0.8*len(corpus_list)):]):\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':dd,\n",
    "            'q_n_text':[''],\n",
    "            'k_text':dd,\n",
    "            'k_n_text':[''],\n",
    "        })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaead27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a9b1b0e",
   "metadata": {},
   "source": [
    "## Generate node classification data for retrieval and reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "880a9470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write labels into documents.json\n",
    "\n",
    "labels_dict = []\n",
    "#for lid in label_name_dict:\n",
    "for lname in label_name2id_dict:\n",
    "    if lname != 'null':\n",
    "        labels_dict.append({'id':label_name2id_dict[lname], 'contents':lname})\n",
    "json.dump(labels_dict, open(f'data_dir/amazon/{sub_dataset}/nc/documents.json', 'w'), indent=4)\n",
    "\n",
    "with open(f'data_dir/amazon/{sub_dataset}/nc/documents.txt', 'w') as fout:\n",
    "    #for lid in label_name_dict:\n",
    "    for lname in label_name2id_dict:\n",
    "        if lname == 'null':\n",
    "            continue\n",
    "        fout.write(str(label_name2id_dict[lname])+'\\t'+lname+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1eb600e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 274354/274354 [00:05<00:00, 48064.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate node query file & ground truth file\n",
    "\n",
    "docid = 0\n",
    "\n",
    "with open(f'data_dir/amazon/{sub_dataset}/nc/node_classification.jsonl') as f, open(f'data_dir/amazon/{sub_dataset}/nc/node_text.tsv', 'w') as fout1, open(f'data_dir/amazon/{sub_dataset}/nc/truth.trec', 'w') as fout2:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        tmp = json.loads(line)\n",
    "        fout1.write(str(docid) + '\\t' + tmp['q_text'] + '\\n')\n",
    "        for label in tmp['labels']:\n",
    "            fout2.write(str(docid)+' '+str(0)+' '+str(label)+' '+str(1)+'\\n')\n",
    "        docid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4423420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219483/219483 [00:14<00:00, 15537.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27435/27435 [00:01<00:00, 14991.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27436/27436 [00:01<00:00, 25996.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate node query file & ground truth file\n",
    "## You can just skip this cell if you want to use bm25 negative\n",
    "\n",
    "docid = 0\n",
    "\n",
    "with open(f'data_dir/amazon/{sub_dataset}/nc/node_classification.jsonl') as f, open(f'data_dir/amazon/{sub_dataset}/nc/train.text.jsonl', 'w') as fout1, open(f'data_dir/amazon/{sub_dataset}/nc/val.text.jsonl', 'w') as fout2, open(f'data_dir/amazon/{sub_dataset}/nc/test.truth.trec', 'w') as fout3, open(f'data_dir/amazon/{sub_dataset}/nc/test.node.text.jsonl', 'w') as fout4:\n",
    "    readin = f.readlines()\n",
    "    total_len = len(readin)\n",
    "    for line in tqdm(readin[:int(0.8*total_len)]):\n",
    "        tmp = json.loads(line)\n",
    "        for label_name in tmp['label_names']:\n",
    "            fout1.write(json.dumps({\n",
    "                'q_text':tmp['q_text'],\n",
    "                'q_n_text':tmp['q_n_text'],\n",
    "                'k_text':label_name,\n",
    "                'k_n_text':[''],\n",
    "            })+'\\n')\n",
    "        docid += 1\n",
    "    \n",
    "    for line in tqdm(readin[int(0.8*total_len):int(0.9*total_len)]):\n",
    "        tmp = json.loads(line)\n",
    "        for label_name in tmp['label_names']:\n",
    "            fout2.write(json.dumps({\n",
    "                'q_text':tmp['q_text'],\n",
    "                'q_n_text':tmp['q_n_text'],\n",
    "                'k_text':label_name,\n",
    "                'k_n_text':[''],\n",
    "            })+'\\n')\n",
    "        docid += 1\n",
    "        \n",
    "    for line in tqdm(readin[int(0.9*total_len):]):\n",
    "        tmp = json.loads(line)\n",
    "        #fout4.write(str(docid) + '\\t' + tmp['q_text'] + '\\n')\n",
    "        fout4.write(json.dumps({\n",
    "                'id': str(docid),\n",
    "                'text':tmp['q_text'],\n",
    "                'n_text':tmp['q_n_text']\n",
    "            })+'\\n')\n",
    "        for label in tmp['labels']:\n",
    "            fout3.write(str(docid)+' '+str(0)+' '+str(label)+' '+str(1)+'\\n')\n",
    "        docid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a10faf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "017a6310",
   "metadata": {},
   "source": [
    "## Generate Coarse-grained Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7930f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 94121.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique labels:16;{6: 'Accessories', 116: 'Action Sports', 27: 'Boating & Water Sports', 2: 'Clothing', 30: 'Cycling', 45: 'Equestrian Sports', 23: 'Exercise & Fitness', 42: 'Fan Shop', 15: 'Golf', 8: 'Hunting & Fishing', 18: 'Leisure Sports & Game Room', 12: 'Outdoor Gear', 154: 'Paintball & Airsoft', 410: 'Racquet Sports', 144: 'Snow Sports', 130: 'Team Sports'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read label name dict\n",
    "coarse_label_id2name = {}\n",
    "#coarse_label_id2idx = {}\n",
    "\n",
    "with open(f'data_dir/amazon/{sub_dataset}/coarse_class.txt') as f:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        tmp = line.strip().split('\\t')\n",
    "        if tmp[1] not in label_name2id_dict:\n",
    "            continue\n",
    "        coarse_label_id2name[label_name2id_dict[tmp[1]]] = tmp[1]\n",
    "\n",
    "print(f'Num of unique labels:{len(coarse_label_id2name)};{coarse_label_id2name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b86921f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 349783/349783 [00:08<00:00, 39081.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique labels:14;{2: 0, 6: 1, 8: 2, 12: 3, 15: 4, 27: 5, 30: 6, 23: 7, 18: 8, 116: 9, 130: 10, 154: 11, 144: 12, 45: 13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate train/val/test file\n",
    "# filter out and only use node which has single label\n",
    "\n",
    "ktrain = 8 # train sample threshold, how many training samples do we have for each class\n",
    "kdev = 8 # dev sample threshold, how many dev samples do we have for each class\n",
    "label_samples = defaultdict(list)\n",
    "\n",
    "with open(f'data_dir/amazon/{sub_dataset}/nc/node_classification.jsonl') as f:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        tmp = json.loads(line)\n",
    "        inter_label = list(set(tmp['labels']) & set(coarse_label_id2name))\n",
    "        if len(inter_label) == 1:\n",
    "            label_samples[inter_label[0]].append(tmp)\n",
    "            \n",
    "# select labels\n",
    "coarse_label_id2idx = {}\n",
    "for l in label_samples:\n",
    "    if len(label_samples[l]) > ktrain + kdev:\n",
    "        coarse_label_id2idx[l] = len(coarse_label_id2idx)\n",
    "        \n",
    "print(f'Num of unique labels:{len(coarse_label_id2idx)};{coarse_label_id2idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eca7d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for coarse-grained classification only\n",
    "random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4089374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "if not os.path.exists(f'data_dir/amazon/{sub_dataset}/nc-coarse/{str(ktrain)}_{str(kdev)}'):\n",
    "    os.mkdir(f'data_dir/amazon/{sub_dataset}/nc-coarse/{str(ktrain)}_{str(kdev)}')\n",
    "\n",
    "with open(f'data_dir/amazon/{sub_dataset}/nc-coarse/{str(ktrain)}_{str(kdev)}/train.text.jsonl', 'w') as fout1, open(f'data_dir/amazon/{sub_dataset}/nc-coarse/{str(ktrain)}_{str(kdev)}/val.text.jsonl', 'w') as fout2, open(f'data_dir/amazon/{sub_dataset}/nc-coarse/{str(ktrain)}_{str(kdev)}/test.text.jsonl', 'w') as fout3:\n",
    "    \n",
    "    assert ktrain+kdev <= 32\n",
    "    \n",
    "    for l in coarse_label_id2idx:\n",
    "        random.shuffle(label_samples[l])\n",
    "        \n",
    "        train_data = label_samples[l][:ktrain]\n",
    "        dev_data = label_samples[l][ktrain:(ktrain+kdev)]\n",
    "        #test_data = label_samples[l][(ktrain+kdev):]\n",
    "        test_data = label_samples[l][16:]\n",
    "    \n",
    "        # write train\n",
    "        for d in train_data:\n",
    "            fout1.write(json.dumps({\n",
    "                'q_text':d['q_text'],\n",
    "                'q_n_text':d['q_n_text'],\n",
    "                'label':coarse_label_id2idx[l]\n",
    "            })+'\\n')\n",
    "    \n",
    "        # write dev\n",
    "        for d in dev_data:\n",
    "            fout2.write(json.dumps({\n",
    "                'q_text':d['q_text'],\n",
    "                'q_n_text':d['q_n_text'],\n",
    "                'label':coarse_label_id2idx[l]\n",
    "            })+'\\n')\n",
    "    \n",
    "        # write test\n",
    "        for d in test_data:\n",
    "            fout3.write(json.dumps({\n",
    "                'q_text':d['q_text'],\n",
    "                'q_n_text':d['q_n_text'],\n",
    "                'label':coarse_label_id2idx[l]\n",
    "            })+'\\n')\n",
    "\n",
    "pickle.dump(coarse_label_id2idx, open(f'data_dir/amazon/{sub_dataset}/nc-coarse/coarse_label_id2idx.pkl', 'wb'))\n",
    "pickle.dump([ktrain, kdev], open(f'data_dir/amazon/{sub_dataset}/nc-coarse/threshold.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de3c430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_label_idx2id = {coarse_label_id2idx[idd]:idd for idd in coarse_label_id2idx}\n",
    "with open(f'data_dir/amazon/{sub_dataset}/nc-coarse/{str(ktrain)}_{str(kdev)}/label_name.txt','w') as fout:\n",
    "    for i in range(len(coarse_label_idx2id)):\n",
    "        fout.write(coarse_label_id2name[coarse_label_idx2id[i]]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d1327a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e3868cd",
   "metadata": {},
   "source": [
    "## Downstream link prediction (co-purchase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c58bc379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 123480/123480 [00:03<00:00, 35504.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test size:1320688,124040,169599\n",
      "Train/Val/Test avg:10.695562034337545,1.0045351473922903,1.3734936831875608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## split train/val/test as 8:1:1\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "train_pairs = []\n",
    "val_pairs = []\n",
    "test_pairs = []\n",
    "train_pair_set = set()\n",
    "\n",
    "for iid in tqdm(also_bought_item):\n",
    "    if iid not in item_id2idx:\n",
    "        continue\n",
    "    \n",
    "    also_bought = also_bought_item[iid]['related']['also_bought']\n",
    "    random.shuffle(also_bought)\n",
    "    \n",
    "    for i in range(int(len(also_bought)*0.8)):\n",
    "        if also_bought[i] in item_id2idx:\n",
    "            train_pairs.append((iid,also_bought[i]))\n",
    "            train_pair_set.add((iid,also_bought[i]))\n",
    "            train_pair_set.add((also_bought[i],iid))\n",
    "\n",
    "    for i in range(int(len(also_bought)*0.8),int(len(also_bought)*0.9)):\n",
    "        if also_bought[i] in item_id2idx:\n",
    "            if (iid,also_bought[i]) in train_pair_set:\n",
    "                continue\n",
    "            val_pairs.append((iid,also_bought[i]))\n",
    "            assert (iid,also_bought[i]) not in train_pair_set\n",
    "\n",
    "    for i in range(int(len(also_bought)*0.9),len(also_bought)):\n",
    "        if also_bought[i] in item_id2idx:\n",
    "            if (iid,also_bought[i]) in train_pair_set:\n",
    "                continue\n",
    "            test_pairs.append((iid,also_bought[i]))\n",
    "            assert (iid,also_bought[i]) not in train_pair_set\n",
    "\n",
    "print(f'Train/Val/Test size:{len(train_pairs)},{len(val_pairs)},{len(test_pairs)}')\n",
    "print(f'Train/Val/Test avg:{len(train_pairs)/len(also_bought_item)},{len(val_pairs)/len(also_bought_item)},{len(test_pairs)/len(also_bought_item)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c850318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1320688/1320688 [05:00<00:00, 4394.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate and save train file\n",
    "\n",
    "random.seed(0)\n",
    "sample_neighbor_num = 5\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/also_bought/train.text.jsonl','w') as fout:\n",
    "    for (q, k) in tqdm(train_pairs):\n",
    "        \n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[q]))\n",
    "        k_n_pool = set(deepcopy(train_neighbor[k]))\n",
    "\n",
    "        if k in q_n_pool:\n",
    "            q_n_pool.remove(k)\n",
    "        if q in k_n_pool:\n",
    "            k_n_pool.remove(q)\n",
    "\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        k_n_pool = list(k_n_pool)\n",
    "        random.shuffle(q_n_pool)\n",
    "        random.shuffle(k_n_pool)\n",
    "        \n",
    "        # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num-len(q_n_pool))\n",
    "        \n",
    "        if len(k_n_pool) >= sample_neighbor_num:\n",
    "            k_samples = k_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            k_samples = k_n_pool + [-1] * (sample_neighbor_num-len(k_n_pool))\n",
    "        \n",
    "        # prepare for writing file\n",
    "        if 'description' in data[q]:\n",
    "            q_text = text_process(data[q]['title'] + ' ' + data[q]['description'])\n",
    "        else:\n",
    "            q_text = text_process(data[q]['title'])\n",
    "        q_n_text = []\n",
    "        for q_n in q_samples:\n",
    "            if q_n == -1:\n",
    "                q_n_text.append('')\n",
    "            elif 'description' in data[q_n]:\n",
    "                q_n_text.append(text_process(data[q_n]['title'] + ' ' + data[q_n]['description']))\n",
    "            else:\n",
    "                q_n_text.append(text_process(data[q_n]['title']))\n",
    "\n",
    "        if 'description' in data[k]:\n",
    "            k_text = text_process(data[k]['title'] + ' ' + data[k]['description'])\n",
    "        else:\n",
    "            k_text = text_process(data[k]['title'])\n",
    "        k_n_text = []\n",
    "        for k_n in k_samples:\n",
    "            if k_n == -1:\n",
    "                k_n_text.append('')\n",
    "            elif 'description' in data[k_n]:\n",
    "                k_n_text.append(text_process(data[k_n]['title'] + ' ' + data[k_n]['description']))\n",
    "            else:\n",
    "                k_n_text.append(text_process(data[k_n]['title']))\n",
    "        \n",
    "        #fout.write(q_line+'\\t'+k_line+'\\n')\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':q_text,\n",
    "            'q_n_text':q_n_text,\n",
    "            'k_text':k_text,\n",
    "            'k_n_text':k_n_text,\n",
    "        })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc6a892f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 124040/124040 [00:20<00:00, 5940.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate and save val file (make sure to delete items that are not in train set)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/also_bought/val.text.jsonl','w') as fout:\n",
    "    for (q, k) in tqdm(val_pairs):\n",
    "        \n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[q]))\n",
    "        k_n_pool = set(deepcopy(train_neighbor[k]))\n",
    "\n",
    "        if k in q_n_pool:\n",
    "            q_n_pool.remove(k)\n",
    "        if q in k_n_pool:\n",
    "            k_n_pool.remove(q)\n",
    "\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        k_n_pool = list(k_n_pool)\n",
    "        random.shuffle(q_n_pool)\n",
    "        random.shuffle(k_n_pool)\n",
    "        \n",
    "        # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num-len(q_n_pool))\n",
    "        \n",
    "        if len(k_n_pool) >= sample_neighbor_num:\n",
    "            k_samples = k_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            k_samples = k_n_pool + [-1] * (sample_neighbor_num-len(k_n_pool))\n",
    "        \n",
    "        # prepare for writing file\n",
    "        if 'description' in data[q]:\n",
    "            q_text = text_process(data[q]['title'] + ' ' + data[q]['description'])\n",
    "        else:\n",
    "            q_text = text_process(data[q]['title'])\n",
    "        q_n_text = []\n",
    "        for q_n in q_samples:\n",
    "            if q_n == -1:\n",
    "                q_n_text.append('')\n",
    "            elif 'description' in data[q_n]:\n",
    "                q_n_text.append(text_process(data[q_n]['title'] + ' ' + data[q_n]['description']))\n",
    "            else:\n",
    "                q_n_text.append(text_process(data[q_n]['title']))\n",
    "\n",
    "        if 'description' in data[k]:\n",
    "            k_text = text_process(data[k]['title'] + ' ' + data[k]['description'])\n",
    "        else:\n",
    "            k_text = text_process(data[k]['title'])\n",
    "        k_n_text = []\n",
    "        for k_n in k_samples:\n",
    "            if k_n == -1:\n",
    "                k_n_text.append('')\n",
    "            elif 'description' in data[k_n]:\n",
    "                k_n_text.append(text_process(data[k_n]['title'] + ' ' + data[k_n]['description']))\n",
    "            else:\n",
    "                k_n_text.append(text_process(data[k_n]['title']))\n",
    "        \n",
    "        #fout.write(q_line+'\\t'+k_line+'\\n')\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':q_text,\n",
    "            'q_n_text':q_n_text,\n",
    "            'k_text':k_text,\n",
    "            'k_n_text':k_n_text,\n",
    "        })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f90ce7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 169599/169599 [00:27<00:00, 6063.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate and save test file (make sure to delete items that are not in train set)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "with open(f'data_dir/{dataset}/{sub_dataset}/also_bought/test.text.jsonl','w') as fout:\n",
    "    for (q, k) in tqdm(test_pairs):\n",
    "        \n",
    "        # prepare sample pool for item\n",
    "        q_n_pool = set(deepcopy(train_neighbor[q]))\n",
    "        k_n_pool = set(deepcopy(train_neighbor[k]))\n",
    "\n",
    "        if k in q_n_pool:\n",
    "            q_n_pool.remove(k)\n",
    "        if q in k_n_pool:\n",
    "            k_n_pool.remove(q)\n",
    "\n",
    "        q_n_pool = list(q_n_pool)\n",
    "        k_n_pool = list(k_n_pool)\n",
    "        random.shuffle(q_n_pool)\n",
    "        random.shuffle(k_n_pool)\n",
    "        \n",
    "        # sample neighbor\n",
    "        if len(q_n_pool) >= sample_neighbor_num:\n",
    "            q_samples = q_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            q_samples = q_n_pool + [-1] * (sample_neighbor_num-len(q_n_pool))\n",
    "        \n",
    "        if len(k_n_pool) >= sample_neighbor_num:\n",
    "            k_samples = k_n_pool[:sample_neighbor_num]\n",
    "        else:\n",
    "            k_samples = k_n_pool + [-1] * (sample_neighbor_num-len(k_n_pool))\n",
    "        \n",
    "        # prepare for writing file\n",
    "        if 'description' in data[q]:\n",
    "            q_text = text_process(data[q]['title'] + ' ' + data[q]['description'])\n",
    "        else:\n",
    "            q_text = text_process(data[q]['title'])\n",
    "        q_n_text = []\n",
    "        for q_n in q_samples:\n",
    "            if q_n == -1:\n",
    "                q_n_text.append('')\n",
    "            elif 'description' in data[q_n]:\n",
    "                q_n_text.append(text_process(data[q_n]['title'] + ' ' + data[q_n]['description']))\n",
    "            else:\n",
    "                q_n_text.append(text_process(data[q_n]['title']))\n",
    "\n",
    "        if 'description' in data[k]:\n",
    "            k_text = text_process(data[k]['title'] + ' ' + data[k]['description'])\n",
    "        else:\n",
    "            k_text = text_process(data[k]['title'])\n",
    "        k_n_text = []\n",
    "        for k_n in k_samples:\n",
    "            if k_n == -1:\n",
    "                k_n_text.append('')\n",
    "            elif 'description' in data[k_n]:\n",
    "                k_n_text.append(text_process(data[k_n]['title'] + ' ' + data[k_n]['description']))\n",
    "            else:\n",
    "                k_n_text.append(text_process(data[k_n]['title']))\n",
    "        \n",
    "        #fout.write(q_line+'\\t'+k_line+'\\n')\n",
    "        fout.write(json.dumps({\n",
    "            'q_text':q_text,\n",
    "            'q_n_text':q_n_text,\n",
    "            'k_text':k_text,\n",
    "            'k_n_text':k_n_text,\n",
    "        })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4d5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
