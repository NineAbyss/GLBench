# @package _global_

defaults:
  - llm_meta_data
# generation hyper-parameters
llm:
  _meta_data: ${_llm_md_lookup.${.base_model}}
  local_dir: /app/GraphText/Llama-2-7b-hf/
  name: LLaMA-PEFT
  base_model: llama2-7b-hf
  hf_name: ${._meta_data.hf_name}
  hidden_dim: 4096

# Train Config
max_length: 1024
max_shard_size: 10GB

max_len: 512
penalty_alpha: 0.6
top_k: 10
top_p: 0.7
random_prefix_len: 5
sample_num: 2
decoding_method: sampling
generate_len: 512



